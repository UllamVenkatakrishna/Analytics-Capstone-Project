{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_vOK3MAwGwH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2LQtTlQ9Ri-O",
        "outputId": "0cdee5a3-c785-4e08-bb88-52335b0f14f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "--- Starting Part A: Unzipping Files ---\n",
            "Found 48 total .zip files to extract.\n",
            "Extracting: 202001-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202002-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202003-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202004-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202005-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202006-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202007-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202008-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202009-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202011-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202010-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n",
            "Extracting: 202012-citibike-tripdata.zip to /content/drive/MyDrive/citi_bike_project/data/2020-citibike-tripdata\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# 1. Connect to your Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- 2. Define your folder paths ---\n",
        "main_data_path = \"/content/drive/MyDrive/citi_bike_project/data/\"\n",
        "processed_data_path = \"/content/drive/MyDrive/citi_bike_project/processed_data/\"\n",
        "os.makedirs(processed_data_path, exist_ok=True)\n",
        "\n",
        "# PART A: UNZIP ALL .ZIP FILES\n",
        "\n",
        "print(\"--- Starting Part A: Unzipping Files ---\")\n",
        "\n",
        "# Use os.walk to find all .zip files in all subdirectories\n",
        "all_zip_files = []\n",
        "for root, dirs, files in os.walk(main_data_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".zip\"):\n",
        "            all_zip_files.append(os.path.join(root, filename))\n",
        "\n",
        "print(f\"Found {len(all_zip_files)} total .zip files to extract.\")\n",
        "\n",
        "# Loop through each found zip file and extract it into its own folder\n",
        "for file_path in all_zip_files:\n",
        "    # The extracted file will go into the same folder where the zip file is.\n",
        "    extract_path = os.path.dirname(file_path)\n",
        "    print(f\"Extracting: {os.path.basename(file_path)} to {extract_path}\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        # Optional: Delete the zip file after successful extraction to save space\n",
        "        # os.remove(file_path)\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\" Warning: Skipping {os.path.basename(file_path)} as it's not a valid zip file.\")\n",
        "\n",
        "print(\"\\n Unzipping complete!\")\n",
        "\n",
        "# PART B: COMBINE ALL .CSV FILES\n",
        "\n",
        "print(\"\\n-Starting Part B: Combining All CSV Files \")\n",
        "\n",
        "# Use os.walk again to find all .csv files now that everything is unzipped\n",
        "all_csv_files = []\n",
        "for root, dirs, files in os.walk(main_data_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".csv\"):\n",
        "            all_csv_files.append(os.path.join(root, filename))\n",
        "\n",
        "print(f\"Found {len(all_csv_files)} total CSV files to combine.\")\n",
        "\n",
        "if not all_csv_files:\n",
        "    print(\"Error: No CSV files were found after unzipping.\")\n",
        "else:\n",
        "    print(\"Loading and combining files... This will take a while. ⏳\")\n",
        "\n",
        "    list_of_dataframes = [pd.read_csv(file, low_memory=False) for file in all_csv_files]\n",
        "    master_df = pd.concat(list_of_dataframes, ignore_index=True)\n",
        "\n",
        "    print(\"Data combination complete!\")\n",
        "    print(f\"Your master dataset has {len(master_df)} rows (trips).\")\n",
        "\n",
        "    # Save the result as a Parquet file ---\n",
        "    output_file = os.path.join(processed_data_path, \"citibike_master.parquet\")\n",
        "    print(f\"\\nSaving combined data to Parquet file: {output_file}\")\n",
        "    master_df.to_parquet(output_file)\n",
        "\n",
        "    print(\"Master file saved successfully! You are now ready for Phase 3.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "\n",
        "# --- 1. Define File Paths ---\n",
        "processed_data_path = \"/content/drive/MyDrive/citi_bike_project/processed_data/\"\n",
        "source_file = os.path.join(processed_data_path, \"citibike_master.parquet\")\n",
        "# This will create a new, uncorrupted version of your cleaned file\n",
        "output_file = os.path.join(processed_data_path, \"citibike_cleaned.parquet\")\n",
        "\n",
        "print(f\"Reading from: {source_file}\")\n",
        "print(f\"Will re-create the cleaned data file at: {output_file}\")\n",
        "\n",
        "# 2. Open the source file and create a writer for the new file ---\n",
        "parquet_file = pq.ParquetFile(source_file)\n",
        "parquet_writer = None\n",
        "\n",
        "# 3. Process the file in chunks ---\n",
        "print(\"\\nStarting cleaning and feature engineering process...\")\n",
        "for i, batch in enumerate(parquet_file.iter_batches(batch_size=1000000)):\n",
        "    print(f\"  -> Processing chunk {i+1}...\")\n",
        "    chunk_df = batch.to_pandas()\n",
        "\n",
        "    # Perform cleaning and feature engineering on the chunk ---\n",
        "    chunk_df['started_at'] = pd.to_datetime(chunk_df['started_at'])\n",
        "    chunk_df['ended_at'] = pd.to_datetime(chunk_df['ended_at'])\n",
        "    chunk_df['hour'] = chunk_df['started_at'].dt.hour\n",
        "    chunk_df['day_of_week'] = chunk_df['started_at'].dt.day_name()\n",
        "    chunk_df['month'] = chunk_df['started_at'].dt.month\n",
        "    chunk_df['trip_duration_minutes'] = (chunk_df['ended_at'] - chunk_df['started_at']).dt.total_seconds() / 60\n",
        "\n",
        "    #Write the processed chunk to the new Parquet file ---\n",
        "    table = pa.Table.from_pandas(chunk_df)\n",
        "    if i == 0:\n",
        "        parquet_writer = pq.ParquetWriter(output_file, table.schema)\n",
        "    parquet_writer.write_table(table)\n",
        "\n",
        "# Close the writer to finalize the file\n",
        "if parquet_writer:\n",
        "    parquet_writer.close()\n",
        "\n",
        "print(\"\\n New 'citibike_cleaned.parquet' file has been created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV239uVhwUDy",
        "outputId": "300d61dc-ca65-448c-f343-c56d0f0f7cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading from: /content/drive/MyDrive/citi_bike_project/processed_data/citibike_master.parquet\n",
            "Will re-create the cleaned data file at: /content/drive/MyDrive/citi_bike_project/processed_data/citibike_cleaned.parquet\n",
            "\n",
            "Starting cleaning and feature engineering process...\n",
            "  -> Processing chunk 1...\n",
            "  -> Processing chunk 2...\n",
            "  -> Processing chunk 3...\n",
            "  -> Processing chunk 4...\n",
            "  -> Processing chunk 5...\n",
            "  -> Processing chunk 6...\n",
            "  -> Processing chunk 7...\n",
            "  -> Processing chunk 8...\n",
            "  -> Processing chunk 9...\n",
            "  -> Processing chunk 10...\n",
            "  -> Processing chunk 11...\n",
            "  -> Processing chunk 12...\n",
            "  -> Processing chunk 13...\n",
            "  -> Processing chunk 14...\n",
            "  -> Processing chunk 15...\n",
            "  -> Processing chunk 16...\n",
            "  -> Processing chunk 17...\n",
            "  -> Processing chunk 18...\n",
            "  -> Processing chunk 19...\n",
            "  -> Processing chunk 20...\n",
            "  -> Processing chunk 21...\n",
            "  -> Processing chunk 22...\n",
            "  -> Processing chunk 23...\n",
            "  -> Processing chunk 24...\n",
            "  -> Processing chunk 25...\n",
            "  -> Processing chunk 26...\n",
            "  -> Processing chunk 27...\n",
            "  -> Processing chunk 28...\n",
            "  -> Processing chunk 29...\n",
            "  -> Processing chunk 30...\n",
            "  -> Processing chunk 31...\n",
            "  -> Processing chunk 32...\n",
            "  -> Processing chunk 33...\n",
            "  -> Processing chunk 34...\n",
            "  -> Processing chunk 35...\n",
            "  -> Processing chunk 36...\n",
            "  -> Processing chunk 37...\n",
            "  -> Processing chunk 38...\n",
            "  -> Processing chunk 39...\n",
            "  -> Processing chunk 40...\n",
            "  -> Processing chunk 41...\n",
            "  -> Processing chunk 42...\n",
            "  -> Processing chunk 43...\n",
            "  -> Processing chunk 44...\n",
            "  -> Processing chunk 45...\n",
            "  -> Processing chunk 46...\n",
            "  -> Processing chunk 47...\n",
            "  -> Processing chunk 48...\n",
            "  -> Processing chunk 49...\n",
            "  -> Processing chunk 50...\n",
            "  -> Processing chunk 51...\n",
            "  -> Processing chunk 52...\n",
            "  -> Processing chunk 53...\n",
            "  -> Processing chunk 54...\n",
            "  -> Processing chunk 55...\n",
            "  -> Processing chunk 56...\n",
            "  -> Processing chunk 57...\n",
            "  -> Processing chunk 58...\n",
            "  -> Processing chunk 59...\n",
            "  -> Processing chunk 60...\n",
            "  -> Processing chunk 61...\n",
            "  -> Processing chunk 62...\n",
            "  -> Processing chunk 63...\n",
            "  -> Processing chunk 64...\n",
            "  -> Processing chunk 65...\n",
            "  -> Processing chunk 66...\n",
            "  -> Processing chunk 67...\n",
            "  -> Processing chunk 68...\n",
            "  -> Processing chunk 69...\n",
            "  -> Processing chunk 70...\n",
            "  -> Processing chunk 71...\n",
            "  -> Processing chunk 72...\n",
            "  -> Processing chunk 73...\n",
            "  -> Processing chunk 74...\n",
            "  -> Processing chunk 75...\n",
            "  -> Processing chunk 76...\n",
            "  -> Processing chunk 77...\n",
            "  -> Processing chunk 78...\n",
            "  -> Processing chunk 79...\n",
            "  -> Processing chunk 80...\n",
            "  -> Processing chunk 81...\n",
            "  -> Processing chunk 82...\n",
            "  -> Processing chunk 83...\n",
            "  -> Processing chunk 84...\n",
            "  -> Processing chunk 85...\n",
            "  -> Processing chunk 86...\n",
            "  -> Processing chunk 87...\n",
            "  -> Processing chunk 88...\n",
            "  -> Processing chunk 89...\n",
            "  -> Processing chunk 90...\n",
            "  -> Processing chunk 91...\n",
            "  -> Processing chunk 92...\n",
            "  -> Processing chunk 93...\n",
            "  -> Processing chunk 94...\n",
            "  -> Processing chunk 95...\n",
            "  -> Processing chunk 96...\n",
            "  -> Processing chunk 97...\n",
            "  -> Processing chunk 98...\n",
            "  -> Processing chunk 99...\n",
            "  -> Processing chunk 100...\n",
            "  -> Processing chunk 101...\n",
            "  -> Processing chunk 102...\n",
            "  -> Processing chunk 103...\n",
            "  -> Processing chunk 104...\n",
            "  -> Processing chunk 105...\n",
            "  -> Processing chunk 106...\n",
            "  -> Processing chunk 107...\n",
            "  -> Processing chunk 108...\n",
            "  -> Processing chunk 109...\n",
            "  -> Processing chunk 110...\n",
            "  -> Processing chunk 111...\n",
            "  -> Processing chunk 112...\n",
            "  -> Processing chunk 113...\n",
            "  -> Processing chunk 114...\n",
            "  -> Processing chunk 115...\n",
            "  -> Processing chunk 116...\n",
            "  -> Processing chunk 117...\n",
            "  -> Processing chunk 118...\n",
            "  -> Processing chunk 119...\n",
            "  -> Processing chunk 120...\n",
            "  -> Processing chunk 121...\n",
            "  -> Processing chunk 122...\n",
            "  -> Processing chunk 123...\n",
            "  -> Processing chunk 124...\n",
            "  -> Processing chunk 125...\n",
            "  -> Processing chunk 126...\n",
            "  -> Processing chunk 127...\n",
            "  -> Processing chunk 128...\n",
            "  -> Processing chunk 129...\n",
            "  -> Processing chunk 130...\n",
            "  -> Processing chunk 131...\n",
            "  -> Processing chunk 132...\n",
            "  -> Processing chunk 133...\n",
            "  -> Processing chunk 134...\n",
            "  -> Processing chunk 135...\n",
            "  -> Processing chunk 136...\n",
            "  -> Processing chunk 137...\n",
            "  -> Processing chunk 138...\n",
            "  -> Processing chunk 139...\n",
            "  -> Processing chunk 140...\n",
            "  -> Processing chunk 141...\n",
            "  -> Processing chunk 142...\n",
            "  -> Processing chunk 143...\n",
            "  -> Processing chunk 144...\n",
            "  -> Processing chunk 145...\n",
            "  -> Processing chunk 146...\n",
            "  -> Processing chunk 147...\n",
            "  -> Processing chunk 148...\n",
            "  -> Processing chunk 149...\n",
            "  -> Processing chunk 150...\n",
            "  -> Processing chunk 151...\n",
            "  -> Processing chunk 152...\n",
            "  -> Processing chunk 153...\n",
            "  -> Processing chunk 154...\n",
            "  -> Processing chunk 155...\n",
            "  -> Processing chunk 156...\n",
            "  -> Processing chunk 157...\n",
            "  -> Processing chunk 158...\n",
            "  -> Processing chunk 159...\n",
            "  -> Processing chunk 160...\n",
            "  -> Processing chunk 161...\n",
            "  -> Processing chunk 162...\n",
            "  -> Processing chunk 163...\n",
            "  -> Processing chunk 164...\n",
            "  -> Processing chunk 165...\n",
            "  -> Processing chunk 166...\n",
            "  -> Processing chunk 167...\n",
            "  -> Processing chunk 168...\n",
            "  -> Processing chunk 169...\n",
            "  -> Processing chunk 170...\n",
            "  -> Processing chunk 171...\n",
            "  -> Processing chunk 172...\n",
            "  -> Processing chunk 173...\n",
            "  -> Processing chunk 174...\n",
            "  -> Processing chunk 175...\n",
            "  -> Processing chunk 176...\n",
            "  -> Processing chunk 177...\n",
            "  -> Processing chunk 178...\n",
            "  -> Processing chunk 179...\n",
            "  -> Processing chunk 180...\n",
            "  -> Processing chunk 181...\n",
            "  -> Processing chunk 182...\n",
            "  -> Processing chunk 183...\n",
            "  -> Processing chunk 184...\n",
            "  -> Processing chunk 185...\n",
            "  -> Processing chunk 186...\n",
            "  -> Processing chunk 187...\n",
            "\n",
            "🚀 New 'citibike_cleaned.parquet' file has been created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJA_HT--hnp0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}